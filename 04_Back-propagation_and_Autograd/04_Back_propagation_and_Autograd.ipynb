{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 04: Back-propagation and Autograd"
      ],
      "metadata": {
        "id": "7PUFJhsekz4D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pXjt3vUbkue3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "w = Variable(torch.Tensor([1.0]), requires_grad=True) # Any random value"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# our model for the forward pass\n",
        "def forward(x):\n",
        "    return x*w\n",
        "\n",
        "# Loss function\n",
        "def loss(x, y):\n",
        "    y_pred = forward(x)\n",
        "    return (y_pred - y) * (y_pred - y)"
      ],
      "metadata": {
        "id": "61wTbI62lRjm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n8tJN8Cl_P5",
        "outputId": "16f57a0b-eedc-4718-bd1c-b8bc860eea1e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqxHITZGl9-7",
        "outputId": "2b2dec98-2aeb-43c6-a39c-43bda6514597"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w.data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDcO_2fmmAZb",
        "outputId": "1cb1812d-cae2-49da-c1f5-c414a1b2b2c0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before training\n",
        "print(\"predict (before training)\", 4, forward(4).data[0])\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(100):\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        l = loss(x_val, y_val)\n",
        "        l.backward()\n",
        "        print(\"\\tgrad: \", x_val, y_val, w.grad.data[0])\n",
        "        w.data = w.data - 0.01 * w.grad.data\n",
        "\n",
        "        # Manually zero the gradients after updating weights\n",
        "        w.grad.data.zero_()\n",
        "\n",
        "    print(f\"Epoch: {epoch}, loss = {l.data[0]}\")\n",
        "\n",
        "# After training\n",
        "print(\"predict (after training)\", \"4 hours\", forward(4).data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Awcwm3H0lZTv",
        "outputId": "17a0ac2a-cace-43a6-a0ec-0cee34e97582"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predict (before training) 4 tensor(4.)\n",
            "\tgrad:  1.0 2.0 tensor(-4.)\n",
            "\tgrad:  2.0 4.0 tensor(-7.6800)\n",
            "\tgrad:  3.0 6.0 tensor(-15.8976)\n",
            "Epoch: 0, loss = 7.02038049697876\n",
            "\tgrad:  1.0 2.0 tensor(-1.4484)\n",
            "\tgrad:  2.0 4.0 tensor(-5.6779)\n",
            "\tgrad:  3.0 6.0 tensor(-11.7533)\n",
            "Epoch: 1, loss = 3.8372161388397217\n",
            "\tgrad:  1.0 2.0 tensor(-1.0709)\n",
            "\tgrad:  2.0 4.0 tensor(-4.1978)\n",
            "\tgrad:  3.0 6.0 tensor(-8.6893)\n",
            "Epoch: 2, loss = 2.0973544120788574\n",
            "\tgrad:  1.0 2.0 tensor(-0.7917)\n",
            "\tgrad:  2.0 4.0 tensor(-3.1034)\n",
            "\tgrad:  3.0 6.0 tensor(-6.4241)\n",
            "Epoch: 3, loss = 1.146377444267273\n",
            "\tgrad:  1.0 2.0 tensor(-0.5853)\n",
            "\tgrad:  2.0 4.0 tensor(-2.2944)\n",
            "\tgrad:  3.0 6.0 tensor(-4.7494)\n",
            "Epoch: 4, loss = 0.6265894174575806\n",
            "\tgrad:  1.0 2.0 tensor(-0.4327)\n",
            "\tgrad:  2.0 4.0 tensor(-1.6963)\n",
            "\tgrad:  3.0 6.0 tensor(-3.5113)\n",
            "Epoch: 5, loss = 0.3424828350543976\n",
            "\tgrad:  1.0 2.0 tensor(-0.3199)\n",
            "\tgrad:  2.0 4.0 tensor(-1.2541)\n",
            "\tgrad:  3.0 6.0 tensor(-2.5960)\n",
            "Epoch: 6, loss = 0.18719476461410522\n",
            "\tgrad:  1.0 2.0 tensor(-0.2365)\n",
            "\tgrad:  2.0 4.0 tensor(-0.9272)\n",
            "\tgrad:  3.0 6.0 tensor(-1.9192)\n",
            "Epoch: 7, loss = 0.10231742262840271\n",
            "\tgrad:  1.0 2.0 tensor(-0.1749)\n",
            "\tgrad:  2.0 4.0 tensor(-0.6855)\n",
            "\tgrad:  3.0 6.0 tensor(-1.4189)\n",
            "Epoch: 8, loss = 0.05592493340373039\n",
            "\tgrad:  1.0 2.0 tensor(-0.1293)\n",
            "\tgrad:  2.0 4.0 tensor(-0.5068)\n",
            "\tgrad:  3.0 6.0 tensor(-1.0490)\n",
            "Epoch: 9, loss = 0.03056734800338745\n",
            "\tgrad:  1.0 2.0 tensor(-0.0956)\n",
            "\tgrad:  2.0 4.0 tensor(-0.3747)\n",
            "\tgrad:  3.0 6.0 tensor(-0.7755)\n",
            "Epoch: 10, loss = 0.01670754700899124\n",
            "\tgrad:  1.0 2.0 tensor(-0.0707)\n",
            "\tgrad:  2.0 4.0 tensor(-0.2770)\n",
            "\tgrad:  3.0 6.0 tensor(-0.5734)\n",
            "Epoch: 11, loss = 0.009132091887295246\n",
            "\tgrad:  1.0 2.0 tensor(-0.0522)\n",
            "\tgrad:  2.0 4.0 tensor(-0.2048)\n",
            "\tgrad:  3.0 6.0 tensor(-0.4239)\n",
            "Epoch: 12, loss = 0.004991436842828989\n",
            "\tgrad:  1.0 2.0 tensor(-0.0386)\n",
            "\tgrad:  2.0 4.0 tensor(-0.1514)\n",
            "\tgrad:  3.0 6.0 tensor(-0.3134)\n",
            "Epoch: 13, loss = 0.0027282594237476587\n",
            "\tgrad:  1.0 2.0 tensor(-0.0286)\n",
            "\tgrad:  2.0 4.0 tensor(-0.1119)\n",
            "\tgrad:  3.0 6.0 tensor(-0.2317)\n",
            "Epoch: 14, loss = 0.0014912093756720424\n",
            "\tgrad:  1.0 2.0 tensor(-0.0211)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0828)\n",
            "\tgrad:  3.0 6.0 tensor(-0.1713)\n",
            "Epoch: 15, loss = 0.0008150564972311258\n",
            "\tgrad:  1.0 2.0 tensor(-0.0156)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0612)\n",
            "\tgrad:  3.0 6.0 tensor(-0.1266)\n",
            "Epoch: 16, loss = 0.00044551375322043896\n",
            "\tgrad:  1.0 2.0 tensor(-0.0115)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0452)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0936)\n",
            "Epoch: 17, loss = 0.0002435151836834848\n",
            "\tgrad:  1.0 2.0 tensor(-0.0085)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0334)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0692)\n",
            "Epoch: 18, loss = 0.00013309309724718332\n",
            "\tgrad:  1.0 2.0 tensor(-0.0063)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0247)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0512)\n",
            "Epoch: 19, loss = 7.274701783899218e-05\n",
            "\tgrad:  1.0 2.0 tensor(-0.0047)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0183)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0378)\n",
            "Epoch: 20, loss = 3.976178413722664e-05\n",
            "\tgrad:  1.0 2.0 tensor(-0.0034)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0135)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0280)\n",
            "Epoch: 21, loss = 2.1730142179876566e-05\n",
            "\tgrad:  1.0 2.0 tensor(-0.0025)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0100)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0207)\n",
            "Epoch: 22, loss = 1.1878906661877409e-05\n",
            "\tgrad:  1.0 2.0 tensor(-0.0019)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0074)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0153)\n",
            "Epoch: 23, loss = 6.490983651019633e-06\n",
            "\tgrad:  1.0 2.0 tensor(-0.0014)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0055)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0113)\n",
            "Epoch: 24, loss = 3.547597771103028e-06\n",
            "\tgrad:  1.0 2.0 tensor(-0.0010)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0040)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0084)\n",
            "Epoch: 25, loss = 1.938678906299174e-06\n",
            "\tgrad:  1.0 2.0 tensor(-0.0008)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0030)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0062)\n",
            "Epoch: 26, loss = 1.0598525932437042e-06\n",
            "\tgrad:  1.0 2.0 tensor(-0.0006)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0022)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0046)\n",
            "Epoch: 27, loss = 5.79895868213498e-07\n",
            "\tgrad:  1.0 2.0 tensor(-0.0004)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0016)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0034)\n",
            "Epoch: 28, loss = 3.165951056871563e-07\n",
            "\tgrad:  1.0 2.0 tensor(-0.0003)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0012)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0025)\n",
            "Epoch: 29, loss = 1.73288071891875e-07\n",
            "\tgrad:  1.0 2.0 tensor(-0.0002)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0009)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0018)\n",
            "Epoch: 30, loss = 9.459313332627062e-08\n",
            "\tgrad:  1.0 2.0 tensor(-0.0002)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0007)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0014)\n",
            "Epoch: 31, loss = 5.173410499992315e-08\n",
            "\tgrad:  1.0 2.0 tensor(-0.0001)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0005)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0010)\n",
            "Epoch: 32, loss = 2.8332806323305704e-08\n",
            "\tgrad:  1.0 2.0 tensor(-9.2030e-05)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0004)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0007)\n",
            "Epoch: 33, loss = 1.548892214486841e-08\n",
            "\tgrad:  1.0 2.0 tensor(-6.7949e-05)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0003)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0006)\n",
            "Epoch: 34, loss = 8.469442036584951e-09\n",
            "\tgrad:  1.0 2.0 tensor(-5.0306e-05)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0002)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0004)\n",
            "Epoch: 35, loss = 4.584762791637331e-09\n",
            "\tgrad:  1.0 2.0 tensor(-3.7193e-05)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0001)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0003)\n",
            "Epoch: 36, loss = 2.5547706172801554e-09\n",
            "\tgrad:  1.0 2.0 tensor(-2.7657e-05)\n",
            "\tgrad:  2.0 4.0 tensor(-0.0001)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0002)\n",
            "Epoch: 37, loss = 1.4190391084412113e-09\n",
            "\tgrad:  1.0 2.0 tensor(-2.0504e-05)\n",
            "\tgrad:  2.0 4.0 tensor(-8.0109e-05)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0002)\n",
            "Epoch: 38, loss = 7.648850441910326e-10\n",
            "\tgrad:  1.0 2.0 tensor(-1.5020e-05)\n",
            "\tgrad:  2.0 4.0 tensor(-5.9128e-05)\n",
            "\tgrad:  3.0 6.0 tensor(-0.0001)\n",
            "Epoch: 39, loss = 4.204139258945361e-10\n",
            "\tgrad:  1.0 2.0 tensor(-1.1206e-05)\n",
            "\tgrad:  2.0 4.0 tensor(-4.3869e-05)\n",
            "\tgrad:  3.0 6.0 tensor(-9.1553e-05)\n",
            "Epoch: 40, loss = 2.3283064365386963e-10\n",
            "\tgrad:  1.0 2.0 tensor(-8.1062e-06)\n",
            "\tgrad:  2.0 4.0 tensor(-3.1471e-05)\n",
            "\tgrad:  3.0 6.0 tensor(-6.2943e-05)\n",
            "Epoch: 41, loss = 1.1004885891452432e-10\n",
            "\tgrad:  1.0 2.0 tensor(-5.9605e-06)\n",
            "\tgrad:  2.0 4.0 tensor(-2.2888e-05)\n",
            "\tgrad:  3.0 6.0 tensor(-4.5776e-05)\n",
            "Epoch: 42, loss = 5.820766091346741e-11\n",
            "\tgrad:  1.0 2.0 tensor(-4.2915e-06)\n",
            "\tgrad:  2.0 4.0 tensor(-1.7166e-05)\n",
            "\tgrad:  3.0 6.0 tensor(-3.7193e-05)\n",
            "Epoch: 43, loss = 3.842615114990622e-11\n",
            "\tgrad:  1.0 2.0 tensor(-3.3379e-06)\n",
            "\tgrad:  2.0 4.0 tensor(-1.3351e-05)\n",
            "\tgrad:  3.0 6.0 tensor(-2.8610e-05)\n",
            "Epoch: 44, loss = 2.2737367544323206e-11\n",
            "\tgrad:  1.0 2.0 tensor(-2.6226e-06)\n",
            "\tgrad:  2.0 4.0 tensor(-1.0490e-05)\n",
            "\tgrad:  3.0 6.0 tensor(-2.2888e-05)\n",
            "Epoch: 45, loss = 1.4551915228366852e-11\n",
            "\tgrad:  1.0 2.0 tensor(-1.9073e-06)\n",
            "\tgrad:  2.0 4.0 tensor(-7.6294e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-1.4305e-05)\n",
            "Epoch: 46, loss = 5.6843418860808015e-12\n",
            "\tgrad:  1.0 2.0 tensor(-1.4305e-06)\n",
            "\tgrad:  2.0 4.0 tensor(-5.7220e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-1.1444e-05)\n",
            "Epoch: 47, loss = 3.637978807091713e-12\n",
            "\tgrad:  1.0 2.0 tensor(-1.1921e-06)\n",
            "\tgrad:  2.0 4.0 tensor(-4.7684e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-1.1444e-05)\n",
            "Epoch: 48, loss = 3.637978807091713e-12\n",
            "\tgrad:  1.0 2.0 tensor(-9.5367e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-3.8147e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-8.5831e-06)\n",
            "Epoch: 49, loss = 2.0463630789890885e-12\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 50, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 51, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 52, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 53, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 54, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 55, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 56, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 57, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 58, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 59, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 60, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 61, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 62, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 63, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 64, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 65, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 66, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 67, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 68, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 69, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 70, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 71, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 72, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 73, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 74, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 75, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 76, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 77, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 78, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 79, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 80, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 81, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 82, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 83, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 84, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 85, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 86, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 87, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 88, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 89, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 90, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 91, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 92, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 93, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 94, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 95, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 96, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 97, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 98, loss = 9.094947017729282e-13\n",
            "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
            "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
            "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
            "Epoch: 99, loss = 9.094947017729282e-13\n",
            "predict (after training) 4 hours tensor(8.0000)\n"
          ]
        }
      ]
    }
  ]
}