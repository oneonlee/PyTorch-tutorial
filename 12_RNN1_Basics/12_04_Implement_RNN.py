# -*- coding: utf-8 -*-
"""PyTorchZeroToAll - Lecture 12_04: Implement RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F7_gJQxHkXj4czkPx9iZCM6Y1ipo4r-l

# Lecture 12: RNN1 - Basics
## Exercise 12-4: Implement RNN
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
import numpy as np

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)

# reproducibility
torch.manual_seed(777)
if device =='cuda':
    torch.cuda.manual_seed_all(777)

"""## (1) Data preparation"""

idx2char = ['h', 'i', 'e', 'l', 'o']

# Teach "hihell" -> "ihello"
x_data = [[0, 1, 0, 2, 3, 3]] # "hihell"
one_hot_lookup = [[[1, 0, 0, 0, 0],  # 0
                   [0, 1, 0, 0, 0],  # 1
                   [0, 0, 1, 0, 0],  # 2
                   [0, 0, 0, 1, 0],  # 3
                   [0, 0, 0, 0, 1]]] # 4

y_data = [1, 0, 2, 3, 3, 4] # "ihello"
x_one_hot = [[one_hot_lookup[0][x] for x in x_data[0]]]

# As we have one batch of samples, we will change them to variables only once
inputs = Variable(torch.Tensor(x_one_hot)).to(device)
labels = Variable(torch.LongTensor(y_data)).to(device)

x_one_hot

"""## (2) Parameters"""

epochs = 100

num_classes = 5
input_size = 5      # one-hot size
hidden_size = 5     # ouput from the LSTM. 5 to directly predict one-hot
batch_size = 1      # one sentence
output_size = 6     # |ihello| == 6
num_layers = 1      # one-layer rnn

"""## (3) Our model

메모리 셀에서 은닉 상태를 계산하는 식을 다음과 같이 정의하였음

$a_{t} = Ux_{t} + Wh_{t−1} + b$<br>
$h_{t} = \tanh{(a_{t})} = \tanh{(Ux_{t} + Wh_{t−1} + b)}$<br>
$o_{t} = Vh_{t} + c$<br>
$\hat{y_{t}}=\textrm{softmax}(o_{t})$<br>
"""

class RNNcell(nn.Module):

    def __init__(self, input_size, hidden_size):
        super(RNNcell, self).__init__()

        # self.input_size = input_size
        self.hidden_size = hidden_size

        self.U = nn.Linear(hidden_size, input_size, bias=True)  # hidden to input (128, 24), bias:b1
        self.W = nn.Linear(hidden_size, hidden_size, bias=True) # hidden to hidden (128, 128), bias:b2
        self.V = nn.Linear(input_size, hidden_size, bias=True)  # input to hidden (24, 128), bias:c
    
        # self.reset_parameters()


    def reset_parameters(self):
        std = 1.0 / np.sqrt(self.hidden_size)

        # Initialize each parameter with numbers sampled from the continuous uniform distribution
        for parameter in self.parameters():
            # type(parameter.data) : Tensor
            parameter.data.uniform_(-std, std)


    def forward(self, x_t, h_prev):
        a_t = self.U(x_t) + self.W(h_prev) # U * x_t + W * h_(t-1) + b
        h_t = torch.tanh(a_t) # tanh{U * x_t + W * h_(t-1) + b}
        o_t = self.V(h_t) # V * h_t + c

        return o_t, h_t # out, hidden


class VanillaRNN(nn.Module):

    def __init__(self, input_size, hidden_size, output_size, num_classes, num_layers=1):
        super(VanillaRNN, self).__init__()

        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.num_classes = num_classes
        self.num_layers = num_layers

        self.rnn = RNNcell(input_size=input_size, hidden_size=hidden_size)


    def forward(self, x):
        h_0 = self.init_hidden(x)

        # Reshape input
        x.view(x.size(0), self.output_size, self.input_size)

        # Propagate input through RNN
        # Input: (batch, seq_len, input_size)
        # h_0: (num_layers * num_directions, batch, hidden_size)

        out, hidden = self.rnn(x, h_0)
        out = out.view(-1, num_classes)
        return out


    def init_hidden(self, x):
        # Initialize hidden and cell states
        return Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device)

class RNNfromPyTorch(nn.Module):

    def __init__(self, input_size, hidden_size, output_size, num_classes, num_layers=1):
        super(RNNfromPyTorch, self).__init__()

        self.num_classes = num_classes
        self.num_layers = num_layers
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        self.rnn = nn.RNN(input_size=5, hidden_size=5, batch_first=True)


    def forward(self, x):
        h_0 = self.init_hidden(x)

        # Reshape input
        x.view(x.size(0), self.output_size, self.input_size)

        # Propagate input through RNN
        # Input: (batch, seq_len, input_size)
        # h_0: (num_layers * num_directions, batch, hidden_size)

        out, hidden = self.rnn(x, h_0)
        out = out.view(-1, num_classes)
        return out

        
    def init_hidden(self, x):
        # Initialize hidden and cell states
        # (num_layers * num_directions, batch, hidden_size) for batch_first=True
        return Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device)

"""## (4) Loss & Training

### VanillaRNN
"""

# Instantiate RNN model
model = VanillaRNN(input_size, hidden_size, output_size, num_classes, num_layers).to(device)
# model = RNNfromPyTorch(input_size, hidden_size, output_size, num_classes, num_layers).to(device)
print(model)

# Set loss and optimizer function
# CrossEntropyLoss = LogSoftmax + NLLLoss
criterion = nn.CrossEntropyLoss().to(device)
optimizer = optim.Adam(model.parameters(), lr=0.1)

# Train the model
for epoch in range(1, epochs + 1):
    outputs = model(inputs).to(device)

    optimizer.zero_grad()
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()

    _, idx = outputs.cpu().max(1)
    idx = idx.data.numpy()
    result_str = "".join([idx2char[c] for c in idx.squeeze()])
    print(f"Epoch: {epoch}, Loss: {loss.data.item()}")
    print(f"Predicted string: {result_str}")

"""### RNNfromPyTorch"""

# Instantiate RNN model
model = RNNfromPyTorch(input_size, hidden_size, output_size, num_classes, num_layers).to(device)
print(model)

# Set loss and optimizer function
# CrossEntropyLoss = LogSoftmax + NLLLoss
criterion = nn.CrossEntropyLoss().to(device)
optimizer = optim.Adam(model.parameters(), lr=0.1)

# Train the model
for epoch in range(1, epochs + 1):
    outputs = model(inputs).to(device)

    optimizer.zero_grad()
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()

    _, idx = outputs.cpu().max(1)
    idx = idx.data.numpy()
    result_str = "".join([idx2char[c] for c in idx.squeeze()])
    print(f"Epoch: {epoch}, Loss: {loss.data.item()}")
    print(f"Predicted string: {result_str}")