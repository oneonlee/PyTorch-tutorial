# -*- coding: utf-8 -*-
"""PyTorchZeroToAll - Lecture 03: Gradient Descent.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kmer2ZQ6HCBlBnmxfb3QT0S9z6AKXjM9

# Lecture 03: Gradient Descent
"""

x_data = [1.0, 2.0, 3.0]
y_data = [2.0, 4.0, 6.0]

w = 1.0 # a random guess: random value

# our model for the forward pass
def forward(x):
    return x*w

# Loss function
def loss(x, y):
    y_pred = forward(x)
    return (y_pred - y) * (y_pred - y)

# compute gradient
def gradient(x, y): # d_loss/d_w
    return 2*x*(x*w-y)

# Before training
print("predict (before training)", 4, forward(4))

# Training Loop
for epoch in range(100):
    for x_val, y_val in zip(x_data, y_data):
        grad = gradient(x_val, y_val)
        w = w - 0.01 * grad
        print("\tgrad: ", x_val, y_val, grad)
        l = loss(x_val, y_val)

    print(f"Epoch: {epoch},\tw = {w}, loss = {l}")

# After training
print("predict (after training)", "4 hours", forward(4))

"""## Exercise 3: Compute gradient and Implement

$$ 
\hat{y}=x^2w_2+xw_1+b \\
loss={(\hat{y}-y)}^2 \\
$$

$$
\frac{\partial loss}{\partial w_1} = ?\\
\frac{\partial loss}{\partial w_2} = ? 
$$
"""

w1 = 1.0 # a random guess: random value
w2 = 2.0 # a random guess: random value

b = 0.5

# our model for the forward pass
def forward(x):
    return x*x*w2+x*w1+b

# compute gradient
def gradient_1(x, y): # d_loss/d_w1
    return 2*x*(x*x*w2+x*w1+b - y)

def gradient_2(x, y): # d_loss/d_w2
    return 2*(x*x)*(x*x*w2+x*w1+b - y)