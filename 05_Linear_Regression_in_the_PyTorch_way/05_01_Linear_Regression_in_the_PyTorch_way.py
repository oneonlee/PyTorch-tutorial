# -*- coding: utf-8 -*-
"""PyTorchZeroToAll - Lecture 05_01: Linear Regression in the PyTorch way.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cElOGEH3gIzA0aSXMM3r1rJ4fJWk3oce

# Lecture 05: Linear Regression in the PyTorch way

## PyTorch Rhythm
1. Design your model using class with Variables
2. Construct your loss and optimizer (select feom PyTorch API)
3. Training cycle (forward, backward, update)

### Data definition (3x1)
"""

import torch
from torch.autograd import Variable

x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0]]))
y_data = Variable(torch.Tensor([[2.0], [4.0], [6.0]]))

"""### (1) Model class in PyTorch way"""

class Model(torch.nn.Module):
    def __init__(self):
        """
        In the constructor we instantiate two nn.Linear module
        """
        super(Model, self).__init__()
        self.linear = torch.nn.Linear(1, 1) # One in and one out

    def forward(self, x):
        """
        In the forward function we accept a Variable of input data 
        and we must return a Variable of output data.
        We can use Modules defined in the constructor as well as arbitrary operators on Variables.
        """
        y_pred = self.linear(x)
        return y_pred

# our model
model = Model()

"""### (2) Construct loss and optimizer"""

# Construct our loss function and an Optimizer.
# The call to model.parameters() in the SGD constructor will contatin the learnable parameters of the two nn.Linear modules which are members of the model.
criterion = torch.nn.MSELoss(reduction='sum')
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

"""### (3) Training: forward, loss, backward, step"""

# Training loop
for epoch in range(500):
    # Forward pass: Compute predicted y passing x to the model
    y_pred = model(x_data)

    # Compute and print loss
    loss = criterion(y_pred, y_data)
    print(f"Epoch : {epoch}\tLoss : {loss.item()}")

    # Zero gradients, perform a backward pass, and update the weights.
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

"""#### Testing"""

# After training
hour_var = Variable(torch.Tensor([[4.0]]))
print(f"predict (after training) = {4} hours : {model.forward(hour_var).data[0][0].item()}")

"""## Exercise: Try other optimizers
- `torch.optim.Adagrad`
- `torch.optim.Adam`
- `torch.optim.Adamax`
- `torch.optim.ASGD`
- `torch.optim.LBFGS`
- `torch.optim.RMSprop`
- `torch.optim.Rprop`
- **`torch.optim.SGD`**

### `torch.optim.Adagrad`
"""

model = Model()
optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)

# Training loop
for epoch in range(500):
    # Forward pass: Compute predicted y passing x to the model
    y_pred = model(x_data)

    # Compute and print loss
    loss = criterion(y_pred, y_data)
    print(f"Epoch : {epoch}\tLoss : {loss.item()}")

    # Zero gradients, perform a backward pass, and update the weights.
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# After training
hour_var = Variable(torch.Tensor([[4.0]]))
print(f"predict (after training) = {4} hours : {model.forward(hour_var).data[0][0].item()}")

"""### `torch.optim.Adam`"""

model = Model()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Training loop
for epoch in range(500):
    # Forward pass: Compute predicted y passing x to the model
    y_pred = model(x_data)

    # Compute and print loss
    loss = criterion(y_pred, y_data)
    print(f"Epoch : {epoch}\tLoss : {loss.item()}")

    # Zero gradients, perform a backward pass, and update the weights.
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# After training
hour_var = Variable(torch.Tensor([[4.0]]))
print(f"predict (after training) = {4} hours : {model.forward(hour_var).data[0][0].item()}")

"""### `torch.optim.Adamax`"""

model = Model()
optimizer = torch.optim.Adamax(model.parameters(), lr=0.01)

# Training loop
for epoch in range(500):
    # Forward pass: Compute predicted y passing x to the model
    y_pred = model(x_data)

    # Compute and print loss
    loss = criterion(y_pred, y_data)
    print(f"Epoch : {epoch}\tLoss : {loss.item()}")

    # Zero gradients, perform a backward pass, and update the weights.
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# After training
hour_var = Variable(torch.Tensor([[4.0]]))
print(f"predict (after training) = {4} hours : {model.forward(hour_var).data[0][0].item()}")

"""### `torch.optim.ASGD`"""

model = Model()
optimizer = torch.optim.ASGD(model.parameters(), lr=0.01)

# Training loop
for epoch in range(500):
    # Forward pass: Compute predicted y passing x to the model
    y_pred = model(x_data)

    # Compute and print loss
    loss = criterion(y_pred, y_data)
    print(f"Epoch : {epoch}\tLoss : {loss.item()}")

    # Zero gradients, perform a backward pass, and update the weights.
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# After training
hour_var = Variable(torch.Tensor([[4.0]]))
print(f"predict (after training) = {4} hours : {model.forward(hour_var).data[0][0].item()}")

"""### `torch.optim.LBFGS`"""

model = Model()
optimizer = torch.optim.LBFGS(model.parameters(), lr=0.01)

# Training loop
for epoch in range(500):
    def closure():
        # Forward pass: Compute predicted y passing x to the model
        y_pred = model(x_data)

        # Compute and print loss
        loss = criterion(y_pred, y_data)
        print(f"Epoch : {epoch}\tLoss : {loss.item()}")

        # Zero gradients, perform a backward pass, and update the weights.
        optimizer.zero_grad()
        loss.backward()
        return loss

    optimizer.step(closure)

# After training
hour_var = Variable(torch.Tensor([[4.0]]))
print(f"predict (after training) = {4} hours : {model.forward(hour_var).data[0][0].item()}")

"""### `torch.optim.RMSprop`"""

model = Model()
optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01)

# Training loop
for epoch in range(500):
    # Forward pass: Compute predicted y passing x to the model
    y_pred = model(x_data)

    # Compute and print loss
    loss = criterion(y_pred, y_data)
    print(f"Epoch : {epoch}\tLoss : {loss.item()}")

    # Zero gradients, perform a backward pass, and update the weights.
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# After training
hour_var = Variable(torch.Tensor([[4.0]]))
print(f"predict (after training) = {4} hours : {model.forward(hour_var).data[0][0].item()}")

"""### `torch.optim.Rprop`

"""

model = Model()
optimizer = torch.optim.Rprop(model.parameters(), lr=0.01)

# Training loop
for epoch in range(500):
    # Forward pass: Compute predicted y passing x to the model
    y_pred = model(x_data)

    # Compute and print loss
    loss = criterion(y_pred, y_data)
    print(f"Epoch : {epoch}\tLoss : {loss.item()}")

    # Zero gradients, perform a backward pass, and update the weights.
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# After training
hour_var = Variable(torch.Tensor([[4.0]]))
print(f"predict (after training) = {4} hours : {model.forward(hour_var).data[0][0].item()}")